import pandas as pd
from snakemake.utils import validate
from scripts.common import read_taxa

validate(config, schema="schemas/config.schema.yaml", set_default=True)
taxa = read_taxa(config)


wildcard_constraints:
    prog="(swarm|opticlust|dbotu3|lulu)",
    tax=f"({'|'.join(taxa)})",
    rundir=config["rundir"],
    run_name=config["run_name"],


container: "docker://continuumio/miniconda3:4.11.0"


include: "rules/common.smk"


if "swarm" in config["software"]:

    include: "rules/swarm.smk"


if "dbotu3" in config["software"]:

    include: "rules/dbotu3.smk"


if "opticlust" in config["software"]:

    include: "rules/opticlust.smk"


if "lulu" in config["software"]:

    include: "rules/lulu.smk"


localrules:
    all,
    precision_recall,
    evaluate,
    merge_rep_files,
    write_asv_reps,


def cluster_files(wildcards):
    input = []
    rundir = config["rundir"]
    for prog in config["software"]:
        run_name = config["run_name"]
        input.append(f"results/{prog}/{rundir}/{run_name}/precision_recall.txt")
    return input


def log_files(wildcards):
    input = []
    rundir = config["rundir"]
    for prog in config["software"]:
        run_name = config["run_name"]
        input.append(f"logs/{prog}/{rundir}/{run_name}/precision_recall.log")
    return input


rule all:
    input:
        expand(
            "results/{tool}/{rundir}/{run_name}/asv_reps.{suff}",
            tool=config["software"],
            rundir=config["rundir"],
            run_name=config["run_name"],
            suff=["taxonomy.tsv", "fasta"],
        ),


rule evaluate:
    input:
        txt=cluster_files,
        log=log_files,
    output:
        expand(
            "results/stats/{rundir}/{run_name}.tsv",
            rundir=config["rundir"],
            run_name=config["run_name"],
        ),
    run:
        import pandas as pd

        stats = {}
        for f in input.txt:
            tool = f.split("/")[1]
            with open(f, "r") as fhin:
                items = {}
                for i, line in enumerate(fhin):
                    items[i] = line.rstrip().split(" ")[-1]
                stats[tool] = {
                    "clusters": items[0],
                    "species": items[1],
                    "precision": items[6].split("\t")[-1],
                    "recall": items[7].split("\t")[-1],
                }
        for f in input.log:
            tool = f.split("/")[1]
            with open(f, "r") as fhin:
                for line in fhin:
                    line = line.rstrip()
                    if line.endswith("ASVs remaining after merging"):
                        stats[tool]["ASVs"] = line.split(" ")[0].lstrip("#")
        df = pd.DataFrame(stats).T
        df.to_csv(output[0], sep="\t")


rule precision_recall:
    input:
        clust_files=expand(
            "results/{{tool}}/{{rundir}}/{tax}/{{run_name}}/asv_clusters.tsv", tax=taxa
        ),
        tax=expand("data/{rundir}/asv_taxa.tsv", rundir=config["rundir"]),
    output:
        "results/{tool}/{rundir}/{run_name}/precision_recall.txt",
    log:
        "logs/{tool}/{rundir}/{run_name}/precision_recall.log",
    params:
        src="workflow/scripts/evaluate_clusters.py",
        eval_rank=config["evaluation_rank"],
    shell:
        """
        python {params.src} {input.tax} {input.clust_files} --rank {params.eval_rank} > {output} 2>{log}
        """


rule calc_colsums:
    output:
        "results/common/{rundir}/colsums.tsv",
    log:
        "logs/common/{rundir}/calc_colsums.log",
    input:
        expand("data/{rundir}/asv_counts.tsv", rundir=config["rundir"]),
    run:
        import pandas as pd

        colsums = []
        data = pd.read_csv(input[0], sep="\t", index_col=0, chunksize=100000)
        for item in data:
            colsums.append(item.sum(axis=0))
        colsums = sum(colsums)
        colsums.to_csv(output[0], sep="\t")


rule write_asv_reps:
    output:
        fasta="results/{tool}/{rundir}/{tax}/{run_name}/asv_reps.fasta",
        taxinfo="results/{tool}/{rundir}/{tax}/{run_name}/asv_reps.taxonomy.tsv",
    log:
        "logs/{tool}/{rundir}/{tax}/{run_name}/write_asv_reps.log",
    input:
        taxa="results/{tool}/{rundir}/{tax}/{run_name}/asv_clusters.tsv",
        extra_taxa=expand("data/{rundir}/asv_taxa.tsv", rundir=config["rundir"]),
        counts="results/common/{rundir}/{tax}/asv_counts.tsv.gz",
        colsums=rules.calc_colsums.output[0],
        seqs="results/common/{rundir}/{tax}/asv_seqs.fasta.gz",
    params:
        src="workflow/scripts/get_asv_reps.py",
        method=config["rep_method"],
        tmpdir="$TMPDIR/{tool}.{rundir}.{tax}.{run_name}.write_asv_reps",
    shell:
        """
        mkdir -p {params.tmpdir}
        gunzip -c {input.counts} > {params.tmpdir}/counts.tsv
        gunzip -c {input.seqs} > {params.tmpdir}/seqs.fasta
        python {params.src} --method {params.method} --prefix {wildcards.tax} --normalize \
            --colsums {input.colsums} --taxa-table {input.extra_taxa} --rank cluster \
            {input.taxa} {params.tmpdir}/counts.tsv {params.tmpdir}/seqs.fasta {output.fasta} 2>{log}
        rm -rf {params.tmpdir}
        """


rule merge_rep_files:
    output:
        taxinfo="results/{tool}/{rundir}/{run_name}/asv_reps.taxonomy.tsv",
        fasta="results/{tool}/{rundir}/{run_name}/asv_reps.fasta",
    input:
        fasta=expand(
            "results/{{tool}}/{{rundir}}/{tax}/{{run_name}}/asv_reps.fasta", tax=taxa
        ),
        taxinfo=expand(
            "results/{{tool}}/{{rundir}}/{tax}/{{run_name}}/asv_reps.taxonomy.tsv",
            tax=taxa,
        ),
    run:
        taxinfo = pd.DataFrame()
        for f in input.taxinfo:
            _df = pd.read_csv(f, sep="\t", index_col=0)
            taxinfo = pd.concat([taxinfo, _df])
        taxinfo.to_csv(output.taxinfo, sep="\t")
        shell("cat {input.fasta} > {output.fasta}")
