import pandas as pd
from snakemake.utils import validate
import re


include: "scripts/common.py"

try:
    validate(config, schema="schemas/config.schema.yaml", set_default=True)
except Exception as e:
    err = e.args[0]
    items = err.split("\n")
    sys.exit(f"""
    ERROR in config file:
    {items[-2]} {items[1]}

    LIKELY CAUSE:
    This error is likely due to a missing or incorrect value in the config file.
    Make sure that no numeric values are quoted and that lists are set to [] when empty.

    WHAT TO DO:
    For example, to set a list parameter to empty, make sure to set it to [] in 
    the config file instead of just commenting out the list items, e.g.:

    This will raise an error
    taxtools:
        #- "sintax"
        #- "vsearch"
        #- "epa-ng"
        #- "sintax+epa-ng"

    Do this instead:
    taxtools: []
    """)

for key in [x for x in config.keys() if x.endswith("-env")]:
    pixi_dir = f".pixi/envs/{key.replace('-env', '')}"
    if os.path.exists(pixi_dir) and os.path.isdir(pixi_dir):
        config[key] = pixi_dir

if config["chimera"]["remove_chimeras"]:
    # If chimera removal is enabled this sets some internal variables
    method = config["chimera"]["method"]
    algorithm = config["chimera"]["algorithm"]
    # The 'chimdir' parameter is created from the method (either 'samplewise'
    # or 'batchwise') and the algorithm (either 'uchime_denovo', 'uchime2_denovo'
    # or 'uchime3_denovo'
    # In addition, the 'chimera_run' string defined by the 'run_name' nested
    # under the chimera settings in the config allows different chimera detection
    # settings using the same vsearch output
    config["chimdir"] = f"{method}.{algorithm}"
else:
    config["chimdir"] = "raw"
    config["chimera"]["run_name"] = "raw"

wildcard_constraints:
    tool="(swarm|opticlust|dbotu3)",
    rundir=config["rundir"],
    run_name=config["run_name"],
    chimdir=config["chimdir"],
    chimera_run=config["chimera"]["run_name"],
    algo=config["chimera"]["algorithm"],

include: "rules/preprocess.smk"
include: "rules/chimeras.smk"
include: "rules/common.smk"
include: "rules/qiime2.smk"
include: "rules/sintax.smk"
include: "rules/epang.smk"
include: "rules/reassign.smk"
include: "rules/swarm.smk"
include: "rules/dbotu3.smk"
include: "rules/opticlust.smk"
include: "rules/neeat.smk"


localrules:
    all,
    precision_recall,
    calc_colsums,
    evaluate,
    aggregate_clusters,
    write_asv_reps,
    sum_cluster_counts,
    write_settings,
    aggregate_consensus_taxonomy,

def eval_input(wildcards):
    input = {"txt": [], "txt_o": [], "log": []}
    rundir = config["rundir"]
    for tool in config["software"]:
        run_name = config["run_name"]
        chimera_run = config["chimera"]["run_name"]
        chimdir = config["chimdir"]
        rank = config["split_rank"]
        input["txt"].append(
            f"results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/precision_recall.txt"
        )
        input["txt_o"].append(
            f"results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/precision_recall.order.txt"
        )
        input["log"].append(
            f"logs/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/precision_recall.log"
        )
    return input

def get_all_taxinput(wildcards):
    input = []
    taxfiles = {'epa-ng': [config["epa-ng"]["msa"], config["epa-ng"]["tree"], config["epa-ng"]["ref_taxonomy"]],
            'sintax': [config["sintax"]["ref"]],
            'vsearch': [config["qiime2"]["ref"], config["qiime2"]["taxfile"]]
            }
    # taxonomy files
    for taxtool in config["taxtools"]:
        if taxtool == "sintax+epa-ng" and all(os.path.exists(x) for x in taxfiles["epa-ng"]+taxfiles["sintax"]):
            input.append(f"results/taxonomy/sintax_epang/{config['rundir']}/{config['epa-ng']['heuristic']}/taxonomy.tsv")
        elif taxtool == "epa-ng" and all(os.path.exists(x) for x in taxfiles["epa-ng"]):
            input.append(f"results/taxonomy/epa-ng/{config['rundir']}/assignments/{config['epa-ng']['heuristic']}/taxonomy.tsv")
        elif taxtool == "sintax" and all(os.path.exists(x) for x in taxfiles["sintax"]):
            input.append(f"results/taxonomy/sintax/{config['rundir']}/taxonomy.tsv")
        elif taxtool == "vsearch" and all(os.path.exists(x) for x in taxfiles["vsearch"]) or all(os.path.exists(x) for x in taxfiles["sintax"]):
            input.append(f"results/taxonomy/vsearch/{config['rundir']}/taxonomy.tsv")
    return input

def get_all_input(wildcards):
    input = []
    input += get_all_taxinput(wildcards)
    # cluster files
    input += expand(
        "results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/cluster_{suff}",
        tool=config["software"],
        rundir=config["rundir"],
        chimdir=config["chimdir"],
        chimera_run=config["chimera"]["run_name"],
        rank=config["split_rank"],
        run_name=config["run_name"],
        suff=["taxonomy.tsv", "reps.fasta", "counts.tsv", "consensus_taxonomy.tsv"],
    )
    # stats files
    input += expand(
        "results/stats/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}.{suff}",
        rundir=config["rundir"],
        chimera_run=config["chimera"]["run_name"],
        chimdir=config["chimdir"],
        rank=config["split_rank"],
        run_name=config["run_name"],
        suff=["tsv", "order.tsv"],
    )
    # settings files
    input += expand(
        "results/settings/{rundir}/{chimera_run}/{chimdir}/{run_name}.{suff}",
        rundir=config["rundir"],
        chimera_run=config["chimera"]["run_name"],
        chimdir=config["chimdir"],
        run_name=config["run_name"],
        suff=["json", "cmd"],
    )
    # noise filtered files
    input += expand(
        "results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/neeat/{noise_rank}/noise_filtered_{f}.tsv",
        tool=config["software"],
        rundir=config["rundir"],
        chimdir=config["chimdir"],
        chimera_run=config["chimera"]["run_name"],
        rank=config["split_rank"],
        run_name=config["run_name"],
        noise_rank=config["noise_filtering"]["split_rank"],
        f=["cluster_counts", "cluster_taxonomy", "cluster_consensus_taxonomy"],
    )
    input += expand(
        "results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/neeat/{noise_rank}/noise_filtered_precision_recall.txt",
        tool=config["software"],
        rundir=config["rundir"],
        chimdir=config["chimdir"],
        chimera_run=config["chimera"]["run_name"],
        rank=config["split_rank"],
        run_name=config["run_name"],
        noise_rank=config["noise_filtering"]["split_rank"],
    )
    return input

rule all:
    input:
        get_all_input

rule preprocess:
    input:
        unpack(get_preprocessed_files)

rule assign_taxonomy:
    input:
        get_all_taxinput

rule evaluate:
    message: "Aggregating evaluations from clustering tools"
    input:
        unpack(eval_input)
    output:
        tsv=expand(
            "results/stats/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}.tsv",
            rundir=config["rundir"],
            chimera_run=config["chimera"]["run_name"],
            chimdir=config["chimdir"],
            rank=config["split_rank"],
            run_name=config["run_name"],
        ),
        ordertsv=expand(
            "results/stats/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}.order.tsv",
            rundir=config["rundir"],
            chimera_run=config["chimera"]["run_name"],
            chimdir=config["chimdir"],
            rank=config["split_rank"],
            run_name=config["run_name"],
        ),
    run:
        import pandas as pd

        stats = {}
        for f in input.txt:
            tool = f.split("/")[1]
            with open(f, "r") as fhin:
                items = {}
                for i, line in enumerate(fhin):
                    items[i] = line.rstrip().split(" ")[-1]
                stats[tool] = {
                    "clusters": items[0],
                    "species": items[1],
                    "precision": items[6].split("\t")[-1],
                    "recall": items[7].split("\t")[-1],
                    "homogeneity": items[8].split("\t")[-1],
                    "completeness": items[9].split("\t")[-1],
                }
        for f in input.log:
            tool = f.split("/")[1]
            with open(f, "r") as fhin:
                for line in fhin:
                    line = line.rstrip()
                    if line.endswith("ASVs remaining after merging"):
                        stats[tool]["ASVs"] = line.split(" ")[0].lstrip("#")
        df = pd.DataFrame(stats).T
        df.to_csv(output.tsv[0], sep="\t")
        orderdf = pd.DataFrame()
        cols = [
            "precision",
            "recall",
            "homogeneity",
            "completeness",
            config["evaluation_rank"],
            "ASVs",
            "tool",
            "clusters",
        ]
        for f in input.txt_o:
            tool = f.split("/")[1]
            _df = pd.read_csv(f, sep="\t", index_col=0)
            _df = _df.assign(tool=pd.Series([tool] * _df.shape[0], index=_df.index))
            _df = _df.loc[:, cols]
            orderdf = pd.concat([orderdf, _df])
        orderdf.to_csv(output.ordertsv[0], sep="\t")

rule clustering:
    input:
        expand(
            "results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/cluster_{suff}",
            tool=config["software"],
            rundir=config["rundir"],
            chimdir=config["chimdir"],
            chimera_run=config["chimera"]["run_name"],
            rank=config["split_rank"],
            run_name=config["run_name"],
            suff=["taxonomy.tsv", "reps.fasta", "counts.tsv", "consensus_taxonomy.tsv"],
        ),
        rules.evaluate.output

rule write_settings:
    message: "Writing configuration parameters to file"
    output:
        json="results/settings/{rundir}/{chimera_run}/{chimdir}/{run_name}.json",
        cmd="results/settings/{rundir}/{chimera_run}/{chimdir}/{run_name}.cmd",
    input:
        get_input_fasta,
    run:
        import json, sys
        with open(output.json, "w") as fhout:
            json.dump(config, fhout, indent=4)
        with open(output.cmd, "w") as fhout:
            fhout.write(" ".join(sys.argv))


def get_asv_clusters(wildcards):
    checkpoint_dir = checkpoints.filter_seqs.get(**wildcards).output[0]
    files = expand(
        "results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/{run_name}/asv_clusters.tsv",
        tool=wildcards.tool,
        rundir=wildcards.rundir,
        chimera_run=wildcards.chimera_run,
        chimdir=wildcards.chimdir,
        rank=wildcards.rank,
        tax=glob_wildcards(os.path.join(checkpoint_dir, "{tax}", "asv_seqs.fasta.gz")).tax,
        run_name=wildcards.run_name,
    )
    return files

rule precision_recall:
    """
    Calculate precision and recall for the clusters
    """
    message: "Calculating precision/recall"
    input:
        clust_files=get_asv_clusters,
        tax=get_input_taxa
    output:
        touch("results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/precision_recall.txt"),
        touch("results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/precision_recall.order.txt"),
    log:
        "logs/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/precision_recall.log",
    params:
        src=workflow.source_path("scripts/evaluate_clusters.py"),
        eval_rank=config["evaluation_rank"],
        split_rank=config["split_rank"]
    shell:
        """
        python {params.src} {input.tax} {input.clust_files} --rank {params.eval_rank} --order_level {output[1]} --cluster_prefix_col {params.split_rank} > {output[0]} 2>{log}
        """

rule calc_colsums:
    """
    Calculate the column sums of the ASVs
    """
    message: "Summing counts per sequence"
    output:
        "results/common/{rundir}/colsums.tsv",
    log:
        "logs/common/{rundir}/calc_colsums.log",
    input:
        expand("data/{rundir}/asv_counts.tsv", rundir=config["rundir"]),
    run:
        import pandas as pd
        colsums = []
        data = pd.read_csv(input[0], sep="\t", index_col=0, chunksize=100000)
        for item in data:
            colsums.append(item.sum(axis=0))
        colsums = sum(colsums)
        colsums.to_csv(output[0], sep="\t")


rule write_asv_reps:
    """
    Writes the representative sequences for each cluster
    """
    message: "Generating representative files for clusters"
    output:
        fasta="results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/{run_name}/cluster_reps.fasta",
        taxinfo="results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/{run_name}/cluster_taxonomy.tsv",
    log:
        "logs/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/{run_name}/write_asv_reps.log",
    input:
        taxa="results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/{run_name}/asv_clusters.tsv",
        extra_taxa=get_input_taxa,
        counts="results/common/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/asv_counts.tsv.gz",
        colsums=rules.calc_colsums.output[0],
        seqs="results/common/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/asv_seqs.fasta.gz",
    params:
        src=workflow.source_path("scripts/get_asv_reps.py"),
        tmpdir="$TMPDIR/{tool}.{rundir}.{chimera_run}.{chimdir}.{rank}.{tax}.{run_name}.write_asv_reps",
    shell:
        """
        mkdir -p {params.tmpdir}
        gunzip -c {input.counts} > {params.tmpdir}/counts.tsv
        gunzip -c {input.seqs} > {params.tmpdir}/seqs.fasta
        python {params.src} --prefix {wildcards.tax} --normalize \
            --colsums {input.colsums} --taxa-table {input.extra_taxa} --rank cluster \
            {input.taxa} {params.tmpdir}/counts.tsv {params.tmpdir}/seqs.fasta {output.fasta} 2>{log}
        rm -rf {params.tmpdir}
        """


rule sum_cluster_counts:
    """
    Sums counts for each cluster
    """
    message: "Summing counts for clusters"
    output:
        "results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/{run_name}/cluster_counts.tsv",
    input:
        taxinfo=rules.write_asv_reps.output.taxinfo,
        counts="results/common/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/asv_counts.tsv.gz"
    run:
        import pandas as pd
        taxdf = pd.read_csv(input.taxinfo, sep="\t", index_col=0)
        countsdf = pd.read_csv(input.counts, sep="\t", index_col=0)
        dataf = pd.merge(
            taxdf.loc[:, ["cluster"]], countsdf, left_index=True, right_index=True
        )
        clustsum = dataf.groupby("cluster").sum()
        clustsum.to_csv(output[0], sep="\t")


def merge_dataframes(input):
    df = pd.DataFrame()
    for i, f in enumerate(input):
        _df = pd.read_csv(f, sep="\t", index_col=0)
        if i == 0:
            columns = _df.columns
        _df = _df[columns]
        df = pd.concat([df, _df])
    return df

def get_clustfiles(wildcards):
    checkpoint_dir = checkpoints.filter_seqs.get(**wildcards).output[0]
    taxa = glob_wildcards(os.path.join(checkpoint_dir, "{tax}", "asv_seqs.fasta.gz")).tax
    base = f"results/{wildcards.tool}/{wildcards.rundir}/{wildcards.chimera_run}/{wildcards.chimdir}/{wildcards.rank}/taxa"
    fasta = expand(os.path.join(base, "{tax}", "{run_name}", "cluster_reps.fasta"),
        tax=taxa, run_name=wildcards.run_name)
    taxinfo = expand(os.path.join(base, "{tax}", "{run_name}", "cluster_taxonomy.tsv"),
        tax=taxa, run_name=wildcards.run_name)
    counts = expand(os.path.join(base, "{tax}", "{run_name}", "cluster_counts.tsv"),
        tax=taxa, run_name=wildcards.run_name)
    return {"taxinfo": taxinfo, "fasta": fasta, "counts": counts}

rule aggregate_clusters:
    """
    Aggregates the cluster files from each taxa
    """
    message: "Aggregating cluster files for {wildcards.tool}"
    output:
        taxinfo="results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/cluster_taxonomy.tsv", # file with cluster membership and taxonomy
        fasta="results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/cluster_reps.fasta",
        counts="results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/cluster_counts.tsv",
    input:
        unpack(get_clustfiles),
    run:
        taxinfo = merge_dataframes(input.taxinfo)
        taxinfo.to_csv(output.taxinfo, sep="\t")
        countsdf = merge_dataframes(input.counts)
        countsdf.to_csv(output.counts, sep="\t")
        shell("cat {input.fasta} > {output.fasta}")

def get_rank_settings(config):
    if config["taxonomy_source"] in ["sintax","sintax+epa-ng"]:
        return "--ranks " + " ".join(config["sintax"]["ranks"])
    elif config["taxonomy_source"] == "epa-ng":
        return "--ranks " + " ".join(config["epa-ng"]["tree_ranks"])
    elif config["taxonomy_source"] == "vsearch":
        if os.path.exists(config["qiime2"]["ref"]) and os.path.exists(config["qiime"]["taxfile"]):
            return "--ranks " + " ".join(config["qiime2"]["ranks"])
        else:
            if os.path.exists(config["sintax"]["ref"]):
                return "--ranks " + " ".join(config["sintax"]["ranks"])
            else:
                return "--ranks"
    elif os.path.isfile(config["taxonomy_source"]):
        return "--ranks " + " ".join(config["ranks"])
    else:
        return "--ranks"

rule consensus_taxonomy:
    """
    Calculates the consensus taxonomy for each cluster per split rank
    """
    message: "Calculating consensus taxonomy for {wildcards.tool} clusters"
    output:
        "results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/{run_name}/cluster_consensus_taxonomy.tsv",
    input:
        clustfile=rules.write_asv_reps.output.taxinfo,
        countsfile="results/common/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/total_counts.tsv"
    log:
        "logs/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/{run_name}/consensus_taxonomy.log",
    params: 
        ranks=get_rank_settings(config),
        cons_ranks="--consensus_ranks " + " ".join(config["consensus_ranks"]),
        thresh=config["consensus_threshold"],
        src=workflow.source_path("scripts/consensus_taxonomy.py")
    shell:
        """
        python {params.src} --countsfile {input.countsfile} \
            --clustfile {input.clustfile} {params[ranks]} {params[cons_ranks]} \
            --consensus_threshold {params[thresh]} > {output} 2>{log}
        """

def get_consensus_taxonomy_files(wildcards):
    checkpoint_dir = checkpoints.filter_seqs.get(**wildcards).output[0]
    files = expand("results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/{run_name}/cluster_consensus_taxonomy.tsv",
        tool=wildcards.tool,
        rundir=wildcards.rundir,
        chimera_run=wildcards.chimera_run,
        chimdir=wildcards.chimdir,
        rank=wildcards.rank,
        tax=glob_wildcards(os.path.join(checkpoint_dir, "{tax}", "asv_seqs.fasta.gz")).tax,
        run_name=wildcards.run_name
        )
    return files

rule aggregate_consensus_taxonomy:
    message: "Aggregating consensus taxonomies"
    output:
        "results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/cluster_consensus_taxonomy.tsv",
    input:
        get_consensus_taxonomy_files,
    log:
        "logs/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/{run_name}/consensus_taxonomy.log",
    run:
        df = merge_dataframes(input)
        df.to_csv(output[0], sep="\t")