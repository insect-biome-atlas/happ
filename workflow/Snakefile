import pandas as pd
from snakemake.utils import validate


include: "scripts/common.py"


validate(config, schema="schemas/config.schema.yaml", set_default=True)

if config["chimera"]["remove_chimeras"]:
    # If config is set to remove chimeras, include the rules file and set
    # additional config params
    method = config["chimera"]["method"]
    algorithm = config["chimera"]["algorithm"]
    # The 'chimdir' parameter is created from the method (either 'samplewise'
    # or 'batchwise') and the algorithm (either 'uchime_denovo', 'uchime2_denovo'
    # or 'uchime3_denovo'
    # In addition, the 'chimera_run' string defined by the 'run_name' nested
    # under the chimera settings in the config allows different chimera detection
    # settings using the same vsearch output
    config["chimdir"] = f"{method}.{algorithm}"
else:
    config["chimdir"] = "raw"
    config["chimera"]["run_name"] = "raw"

wildcard_constraints:
    tool="(swarm|opticlust|dbotu3)",
    rundir=config["rundir"],
    run_name=config["run_name"],
    chimdir=config["chimdir"],
    chimera_run=config["chimera"]["run_name"],
    algo=config["chimera"]["algorithm"],
    noise_run=config["noise_filtering"]["run_name"],

include: "rules/preprocess.smk"
include: "rules/chimeras.smk"
include: "rules/common.smk"
include: "rules/qiime2.smk"
include: "rules/sintax.smk"
include: "rules/swarm.smk"
include: "rules/dbotu3.smk"
include: "rules/opticlust.smk"
include: "rules/neeat.smk"

localrules:
    all,
    precision_recall,
    calc_colsums,
    evaluate,
    aggregate_clusters,
    write_asv_reps,
    sum_cluster_counts,
    write_settings,
    aggregate_consensus_taxonomy,


def cluster_files(wildcards):
    input = []
    rundir = config["rundir"]
    for tool in config["software"]:
        run_name = config["run_name"]
        chimera_run = config["chimera"]["run_name"]
        chimdir = config["chimdir"]
        rank = config["split_rank"]
        input.append(
            f"results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/precision_recall.txt"
        )
    return input


def cluster_order_files(wildcards):
    input = []
    rundir = config["rundir"]
    for tool in config["software"]:
        run_name = config["run_name"]
        chimera_run = config["chimera"]["run_name"]
        chimdir = config["chimdir"]
        rank = config["split_rank"]
        input.append(
            f"results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/precision_recall.order.txt"
        )
    return input


def log_files(wildcards):
    input = []
    rundir = config["rundir"]
    for tool in config["software"]:
        run_name = config["run_name"]
        chimera_run = config["chimera"]["run_name"]
        rank = config["split_rank"]
        chimdir = config["chimdir"]
        input.append(
            f"logs/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/precision_recall.log"
        )
    return input

def get_all_input(wildcards):
    input = []
    # cluster files
    input += expand(
        "results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/cluster_{suff}",
        tool=config["software"],
        rundir=config["rundir"],
        chimdir=config["chimdir"],
        chimera_run=config["chimera"]["run_name"],
        rank=config["split_rank"],
        run_name=config["run_name"],
        suff=["taxonomy.tsv", "reps.fasta", "counts.tsv", "consensus_taxonomy.tsv"],
    )
    # stats files
    input += expand(
        "results/stats/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}.{suff}",
        rundir=config["rundir"],
        chimera_run=config["chimera"]["run_name"],
        chimdir=config["chimdir"],
        rank=config["split_rank"],
        run_name=config["run_name"],
        suff=["tsv", "order.tsv"],
    )
    # settings files
    input += expand(
        "results/settings/{rundir}/{chimera_run}/{chimdir}/{run_name}.{suff}",
        rundir=config["rundir"],
        chimera_run=config["chimera"]["run_name"],
        chimdir=config["chimdir"],
        run_name=config["run_name"],
        suff=["json", "cmd"],
    )
    # noise filtered files
    input += rules.noise_filtering.input
    # cleaning files
    if os.path.exists(config["cleaning"]["metadata_file"]):
        input += rules.cleaning.input
    return input


rule all:
    input:
        get_all_input

rule evaluate:
    input:
        txt=cluster_files,
        txt_o=cluster_order_files,
        log=log_files,
    output:
        tsv=expand(
            "results/stats/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}.tsv",
            rundir=config["rundir"],
            chimera_run=config["chimera"]["run_name"],
            chimdir=config["chimdir"],
            rank=config["split_rank"],
            run_name=config["run_name"],
        ),
        ordertsv=expand(
            "results/stats/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}.order.tsv",
            rundir=config["rundir"],
            chimera_run=config["chimera"]["run_name"],
            chimdir=config["chimdir"],
            rank=config["split_rank"],
            run_name=config["run_name"],
        ),
    run:
        import pandas as pd

        stats = {}
        for f in input.txt:
            tool = f.split("/")[1]
            with open(f, "r") as fhin:
                items = {}
                for i, line in enumerate(fhin):
                    items[i] = line.rstrip().split(" ")[-1]
                stats[tool] = {
                    "clusters": items[0],
                    "species": items[1],
                    "precision": items[6].split("\t")[-1],
                    "recall": items[7].split("\t")[-1],
                    "homogeneity": items[8].split("\t")[-1],
                    "completeness": items[9].split("\t")[-1],
                }
        for f in input.log:
            tool = f.split("/")[1]
            with open(f, "r") as fhin:
                for line in fhin:
                    line = line.rstrip()
                    if line.endswith("ASVs remaining after merging"):
                        stats[tool]["ASVs"] = line.split(" ")[0].lstrip("#")
        df = pd.DataFrame(stats).T
        df.to_csv(output.tsv[0], sep="\t")
        orderdf = pd.DataFrame()
        cols = [
            "precision",
            "recall",
            "homogeneity",
            "completeness",
            config["evaluation_rank"],
            "ASVs",
            "tool",
            "clusters",
        ]
        for f in input.txt_o:
            tool = f.split("/")[1]
            _df = pd.read_csv(f, sep="\t", index_col=0)
            _df = _df.assign(tool=pd.Series([tool] * _df.shape[0], index=_df.index))
            _df = _df.loc[:, cols]
            orderdf = pd.concat([orderdf, _df])
        orderdf.to_csv(output.ordertsv[0], sep="\t")

rule clustering:
    input:
        expand(
            "results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/cluster_{suff}",
            tool=config["software"],
            rundir=config["rundir"],
            chimdir=config["chimdir"],
            chimera_run=config["chimera"]["run_name"],
            rank=config["split_rank"],
            run_name=config["run_name"],
            suff=["taxonomy.tsv", "reps.fasta", "counts.tsv", "consensus_taxonomy.tsv"],
        ),
        rules.evaluate.output

rule write_settings:
    output:
        json="results/settings/{rundir}/{chimera_run}/{chimdir}/{run_name}.json",
        cmd="results/settings/{rundir}/{chimera_run}/{chimdir}/{run_name}.cmd",
    input:
        get_input_fasta,
    run:
        import json, sys

        with open(output.json, "w") as fhout:
            json.dump(config, fhout, indent=4)
        with open(output.cmd, "w") as fhout:
            fhout.write(" ".join(sys.argv))


def get_asv_clusters(wildcards):
    checkpoint_dir = checkpoints.filter_seqs.get(**wildcards).output[0]
    files = expand(
        "results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/{run_name}/asv_clusters.tsv",
        tool=wildcards.tool,
        rundir=wildcards.rundir,
        chimera_run=wildcards.chimera_run,
        chimdir=wildcards.chimdir,
        rank=wildcards.rank,
        tax=glob_wildcards(os.path.join(checkpoint_dir, "{tax}", "asv_seqs.fasta.gz")).tax,
        run_name=wildcards.run_name,
    )
    return files

rule precision_recall:
    """
    Calculate precision and recall for the clusters
    """
    input:
        clust_files=get_asv_clusters,
        tax=get_input_taxa
    output:
        "results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/precision_recall.txt",
        "results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/precision_recall.order.txt",
    log:
        "logs/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/precision_recall.log",
    params:
        src=workflow.source_path("scripts/evaluate_clusters.py"),
        eval_rank=config["evaluation_rank"],
    shell:
        """
        python {params.src} {input.tax} {input.clust_files} --rank {params.eval_rank} --order_level {output[1]} > {output[0]} 2>{log}
        """

rule calc_colsums:
    """
    Calculate the column sums of the ASVs
    """
    output:
        "results/common/{rundir}/colsums.tsv",
    log:
        "logs/common/{rundir}/calc_colsums.log",
    input:
        expand("data/{rundir}/asv_counts.tsv", rundir=config["rundir"]),
    run:
        import pandas as pd
        colsums = []
        data = pd.read_csv(input[0], sep="\t", index_col=0, chunksize=100000)
        for item in data:
            colsums.append(item.sum(axis=0))
        colsums = sum(colsums)
        colsums.to_csv(output[0], sep="\t")


rule write_asv_reps:
    """
    Writes the representative sequences for each cluster
    """
    output:
        fasta="results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/{run_name}/cluster_reps.fasta",
        taxinfo="results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/{run_name}/cluster_taxonomy.tsv",
    log:
        "logs/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/{run_name}/write_asv_reps.log",
    input:
        taxa="results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/{run_name}/asv_clusters.tsv",
        extra_taxa=get_input_taxa,
        counts="results/common/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/asv_counts.tsv.gz",
        colsums=rules.calc_colsums.output[0],
        seqs="results/common/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/asv_seqs.fasta.gz",
    params:
        src=workflow.source_path("scripts/get_asv_reps.py"),
        tmpdir="$TMPDIR/{tool}.{rundir}.{chimera_run}.{chimdir}.{rank}.{tax}.{run_name}.write_asv_reps",
    shell:
        """
        mkdir -p {params.tmpdir}
        gunzip -c {input.counts} > {params.tmpdir}/counts.tsv
        gunzip -c {input.seqs} > {params.tmpdir}/seqs.fasta
        python {params.src} --prefix {wildcards.tax} --normalize \
            --colsums {input.colsums} --taxa-table {input.extra_taxa} --rank cluster \
            {input.taxa} {params.tmpdir}/counts.tsv {params.tmpdir}/seqs.fasta {output.fasta} 2>{log}
        rm -rf {params.tmpdir}
        """

rule sum_cluster_counts:
    """
    Sums counts for each cluster
    """
    output:
        "results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/{run_name}/cluster_counts.tsv",
    input:
        taxinfo=rules.write_asv_reps.output.taxinfo,
        counts="results/common/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/asv_counts.tsv.gz"
    run:
        import pandas as pd
        taxdf = pd.read_csv(input.taxinfo, sep="\t", index_col=0)
        countsdf = pd.read_csv(input.counts, sep="\t", index_col=0)
        dataf = pd.merge(
            taxdf.loc[:, ["cluster"]], countsdf, left_index=True, right_index=True
        )
        clustsum = dataf.groupby("cluster").sum()
        clustsum.to_csv(output[0], sep="\t")


def merge_dataframes(input):
    df = pd.DataFrame()
    for i, f in enumerate(input):
        _df = pd.read_csv(f, sep="\t", index_col=0)
        if i == 0:
            columns = _df.columns
        _df = _df[columns]
        df = pd.concat([df, _df])
    return df

def get_clustfiles(wildcards):
    checkpoint_dir = checkpoints.filter_seqs.get(**wildcards).output[0]
    taxa = glob_wildcards(os.path.join(checkpoint_dir, "{tax}", "asv_seqs.fasta.gz")).tax
    base = f"results/{wildcards.tool}/{wildcards.rundir}/{wildcards.chimera_run}/{wildcards.chimdir}/{wildcards.rank}/taxa"
    fasta = expand(os.path.join(base, "{tax}", "{run_name}", "cluster_reps.fasta"),
        tax=taxa, run_name=wildcards.run_name)
    taxinfo = expand(os.path.join(base, "{tax}", "{run_name}", "cluster_taxonomy.tsv"),
        tax=taxa, run_name=wildcards.run_name)
    counts = expand(os.path.join(base, "{tax}", "{run_name}", "cluster_counts.tsv"),
        tax=taxa, run_name=wildcards.run_name)
    return {"taxinfo": taxinfo, "fasta": fasta, "counts": counts}

rule aggregate_clusters:
    """
    Aggregates the cluster files from each taxa
    """
    output:
        taxinfo="results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/cluster_taxonomy.tsv", # file with cluster membership and taxonomy
        fasta="results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/cluster_reps.fasta",
        counts="results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/cluster_counts.tsv",
    input:
        unpack(get_clustfiles),
    run:
        taxinfo = merge_dataframes(input.taxinfo)
        taxinfo.to_csv(output.taxinfo, sep="\t")
        countsdf = merge_dataframes(input.counts)
        countsdf.to_csv(output.counts, sep="\t")
        shell("cat {input.fasta} > {output.fasta}")

rule consensus_taxonomy:
    """
    Calculates the consensus taxonomy for each cluster per split rank
    """
    output:
        "results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/{run_name}/cluster_consensus_taxonomy.tsv",
    input:
        clustfile=rules.write_asv_reps.output.taxinfo,
        countsfile="results/common/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/total_counts.tsv"
    log:
        "logs/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/{run_name}/consensus_taxonomy.log",
    params:
        ranks = config["ranks"],
        cons_ranks = config["consensus_ranks"],
        thresh = config["consensus_threshold"],
    group: "consensus_taxonomy"
    shell:
        """
        python workflow/scripts/consensus_taxonomy.py --countsfile {input.countsfile} \
            --clustfile {input.clustfile} --ranks {params.ranks} --consensus_ranks {params.cons_ranks} \
            --consensus_threshold {params.thresh} > {output} 2>{log}
        """

def get_consensus_taxonomy_files(wildcards):
    checkpoint_dir = checkpoints.filter_seqs.get(**wildcards).output[0]
    files = expand("results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/taxa/{tax}/{run_name}/cluster_consensus_taxonomy.tsv",
        tool=wildcards.tool,
        rundir=wildcards.rundir,
        chimera_run=wildcards.chimera_run,
        chimdir=wildcards.chimdir,
        rank=wildcards.rank,
        tax=glob_wildcards(os.path.join(checkpoint_dir, "{tax}", "asv_seqs.fasta.gz")).tax,
        run_name=wildcards.run_name
        )
    return files

rule aggregate_consensus_taxonomy:
    output:
        "results/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/runs/{run_name}/cluster_consensus_taxonomy.tsv",
    input:
        get_consensus_taxonomy_files,
    log:
        "logs/{tool}/{rundir}/{chimera_run}/{chimdir}/{rank}/{run_name}/consensus_taxonomy.log",
    run:
        df = merge_dataframes(input)
        df.to_csv(output[0], sep="\t")