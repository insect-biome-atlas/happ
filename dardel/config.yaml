# This is the configuration profile for the Dardel HPC system
# Read more about the available partitions at https://www.pdc.kth.se/support/documents/run_jobs/job_scheduling.html#dardel-partitions
# and about the compute nodes at https://www.pdc.kth.se/support/documents/run_jobs/job_scheduling.html#dardel-compute-nodes

# Update <your-slurm-account> under default-resources to your own slurm compute account
default-resources: 
  slurm_account: <your-slurm-account>
  slurm_partition: shared
  runtime: 1440

# The settings below override the default-resources for specific rules
set-resources:
  sum_asvs:
    runtime: 60
    mem_mb: 60000
  chimera_samplewise:
    runtime: 60
    mem_mb: 24000
  vsearch_align:
    mem_mb: 60000
  run_dbotu3:
    runtime: 10080
    slurm_partition: long
    mem_mb: 24000
  mafft_align:
    runtime: 120
    mem_mb: 24000
  pal2nal:
    runtime: 120
  run_opticlust:
    mem_mb: 60000
  split_counts_samplewise:
    runtime: 20
  consensus_taxonomy:
    runtime: 60
  id_spikeins:
    runtime: 120
    mem_mb: 60000
  clean_asvs:
    mem_mb: 60000
  lulu_matchlist:
    runtime: 120

set-threads:
  vsearch_align: 40
  mafft_align: 40
  mothur_align: 40
  run_opticlust: 40
  run_swarm: 40
  sum_asvs: 1
  split_counts: 1
  chimera_samplewise: 40
  id_spikeins: 10
  lulu_matchlist: 40

# The settings below define command line flags for snakemake
keep-going: True
printshellcmds: True
rerun-triggers: mtime
rerun-incomplete: True
local-cores: 1
jobs: 100
latency-wait: 5
executor: slurm
software-deployment-method: "conda"